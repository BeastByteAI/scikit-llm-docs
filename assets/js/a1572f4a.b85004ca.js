"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[769],{6928:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var o=t(5893),a=t(1151);const i={sidebar_position:2},l="Backend Families",s={id:"backend-families",title:"Backend Families",description:"On a high level, Scikit-LLM estimators are divided based on the language model backend family they use. The backend family is defined by the API format and does not necessarily correspond to the language model architecture. For example, all backends that follow the OpenAI API format are groupped into gpt family regardless the actual language model architecture or provider. Eeach backend family has its own set of estimators which are located in the skllm.models. sub-module.",source:"@site/docs/backend-families.md",sourceDirName:".",slug:"/backend-families",permalink:"/docs/backend-families",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Quick Start",permalink:"/docs/intro"},next:{title:"Text Classification",permalink:"/docs/category/text-classification"}},r={},d=[{value:"GPT Family",id:"gpt-family",level:2},{value:"OpenAI (default)",id:"openai-default",level:3},{value:"Azure",id:"azure",level:3},{value:"GPT4ALL",id:"gpt4all",level:3},{value:"Custom URL",id:"custom-url",level:3},{value:"Vertex Family",id:"vertex-family",level:2}];function c(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"backend-families",children:"Backend Families"}),"\n",(0,o.jsxs)(n.p,{children:["On a high level, Scikit-LLM estimators are divided based on the language model backend family they use. The backend family is defined by the API format and does not necessarily correspond to the language model architecture. For example, all backends that follow the OpenAI API format are groupped into ",(0,o.jsx)(n.em,{children:"gpt"})," family regardless the actual language model architecture or provider. Eeach backend family has its own set of estimators which are located in the ",(0,o.jsx)(n.code,{children:"skllm.models.<family>"})," sub-module."]}),"\n",(0,o.jsxs)(n.p,{children:["For example, the Zero-Shot Classifier is available as ",(0,o.jsx)(n.code,{children:"skllm.models.gpt.classification.zero_shot.ZeroShotGPTClassifier"})," for the ",(0,o.jsx)(n.em,{children:"gpt"})," family, and as ",(0,o.jsx)(n.code,{children:"skllm.models.vertex.classification.zero_shot.ZeroShotVertexClassifier"})," for the ",(0,o.jsx)(n.em,{children:"vertex"})," family. The separation between the backend families is necessary to allow for a reasonble level of flexibility if/when model providers introduce model-specific features that are not supported by other providers and hence cannot be easily abstracted away. At the same time, the number of model families is kept to a minimum to simplify the usage and maintenance of the library. Since the OpenAI API is by far the most popular and widely used, backends that follow that format are preffered over the others."]}),"\n",(0,o.jsxs)(n.p,{children:["Whenever the backend family supports multiple backends, the default one is used unless the ",(0,o.jsx)(n.code,{children:"model"})," parameter specifies a particular backend namespace. For example, the default backend for the ",(0,o.jsx)(n.em,{children:"gpt"})," family is the OpenAI backend. However, you can use the Azure backend by setting ",(0,o.jsx)(n.code,{children:'model = "azure::<model_name>"'}),". However, please note that not every estimator supports every backend."]}),"\n",(0,o.jsx)(n.h2,{id:"gpt-family",children:"GPT Family"}),"\n",(0,o.jsx)(n.p,{children:"The GPT family includes all backends that follow the OpenAI API format."}),"\n",(0,o.jsx)(n.h3,{id:"openai-default",children:"OpenAI (default)"}),"\n",(0,o.jsxs)(n.p,{children:["The OpenAI backend is the default backend for the GPT family. It is used whenever the ",(0,o.jsx)(n.code,{children:"model"})," parameter does not specify a particular backend namespace."]}),"\n",(0,o.jsx)(n.p,{children:"To use the OpenAI backend, you need to set your OpenAI API key and organization ID as follows:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from skllm.config import SKLLMConfig\n\nSKLLMConfig.set_openai_key("<YOUR_KEY>")\nSKLLMConfig.set_openai_org("<YOUR_ORGANIZATION_ID>")\n'})}),"\n",(0,o.jsx)(n.h3,{id:"azure",children:"Azure"}),"\n",(0,o.jsxs)(n.p,{children:["OpenAI models can be alternatively used as a part of the ",(0,o.jsx)(n.a,{href:"https://azure.microsoft.com/en-us/products/ai-services/openai-service",children:"Azure OpenAI service"}),". To use the Azure backend, you need to provide your Azure API key and endpoint as follows:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from skllm.config import SKLLMConfig\n# Found under: Resource Management (Left Sidebar) -> Keys and Endpoint -> KEY 1\nSKLLMConfig.set_gpt_key("<YOUR_KEY>")\n# Found under: Resource Management (Left Sidebar) -> Keys and Endpoint -> Endpoint\nSKLLMConfig.set_azure_api_base("<API_BASE>") # e.g. https://<YOUR_PROJECT_NAME>.openai.azure.com/\n'})}),"\n",(0,o.jsxs)(n.p,{children:["When using the Azure backend, the model should be specified as ",(0,o.jsx)(n.code,{children:'model = "azure::<model_deployment_name>"'}),". For example, if you created a ",(0,o.jsx)(n.em,{children:"gpt-3.5"})," deployment under the name ",(0,o.jsx)(n.em,{children:"my-model"}),", you should use ",(0,o.jsx)(n.code,{children:'model = "azure::my-model"'}),"."]}),"\n",(0,o.jsx)(n.h3,{id:"gpt4all",children:"GPT4ALL"}),"\n",(0,o.jsx)(n.p,{children:"GPT4ALL is an open-source library that provides a unified API for multiple small-scale language models, that can be run locally on a consumer-grade hardware, even without a GPU. To use the GPT4ALL backend, you need to install the corresponding extension as follows:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"pip install scikit-llm[gpt4all]\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Then, you can use the GPT4ALL by specifying the model as ",(0,o.jsx)(n.code,{children:'model = "gpt4all::<model_name>"'}),", which will be downloaded automatically. For the full list of available models, please refer to the ",(0,o.jsx)(n.a,{href:"https://gpt4all.io/index.html",children:"GPT4ALL official documentation"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Note:"})," The models available through the GPT4ALL out of the box have very limited capabilities and are not recommended for most of the use cases. In addition, not all models are permitted for commercial use. Please check the license of the model you are using before deploying it in production."]}),"\n",(0,o.jsx)(n.h3,{id:"custom-url",children:"Custom URL"}),"\n",(0,o.jsx)(n.p,{children:"Not available yet. Expected in the next release."}),"\n",(0,o.jsx)(n.h2,{id:"vertex-family",children:"Vertex Family"}),"\n",(0,o.jsx)(n.p,{children:"The Vertex family currently includes a single (default) backend, which is the Google Vertex AI."}),"\n",(0,o.jsx)(n.p,{children:"In order to use the Vertex backend, you need to configure your Google Cloud credentials as follows:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Log in to ",(0,o.jsx)(n.a,{href:"https://console.cloud.google.com/",children:"Google Cloud Console"})," and ",(0,o.jsx)(n.a,{href:"https://developers.google.com/workspace/guides/create-project",children:"create a Google Cloud project"}),". After the project is created, select this project from a list of projects next to the Google Cloud logo (upper left corner)."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Search for ",(0,o.jsx)(n.em,{children:"Vertex AI"})," in the search bar and select it from the list of services."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Install a Google Cloud CLI on the local machine by following ",(0,o.jsx)(n.a,{href:"https://cloud.google.com/sdk/docs/install",children:"the steps from the official documentation"}),", and ",(0,o.jsx)(n.a,{href:"https://cloud.google.com/docs/authentication/application-default-credentials#personal",children:"set the application default credentials"})," by running the following command:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"gcloud auth application-default login\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Configure Scikit-LLM with your project ID:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from skllm.config import SKLLMConfig\n\nSKLLMConfig.set_google_project("<YOUR_PROJECT_ID>")\n'})}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Additionally, for tuning LLMs in Vertex, it is required to have to have 64 cores of the TPU v3 pod training resource. By default this quota is set to 0 cores and has to be increased as follows (ignore this if you are not planning to use the tunable estimators):"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Go to ",(0,o.jsx)(n.a,{href:"https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota",children:"Quotas"})," and filter them for \u201cRestricted image training TPU V3 pod cores per region\u201d."]}),"\n",(0,o.jsx)(n.li,{children:"Select \u201ceurope-west4\u201d region (currently this is the only supported region)."}),"\n",(0,o.jsx)(n.li,{children:"Click on \u201cEdit Quotas\u201d, set the limit to 64 and submit the request.\nThe request should be approved within a few hours, but it might take up to several days."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},1151:(e,n,t)=>{t.d(n,{Z:()=>s,a:()=>l});var o=t(7294);const a={},i=o.createContext(a);function l(e){const n=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:l(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);